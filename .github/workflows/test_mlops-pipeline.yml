name: Test MLOps Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: "0 0 * * 0"

jobs:
  test-infrastructure:
    name: Test MLOps Infrastructure
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
          pip install pytest pytest-cov

      - name: Verify directory structure
        run: |
          python -c "
          import os, sys
          required_dirs = ['src', 'src/scripts', 'src/tests', 'models', 'data', 'results']
          missing = [d for d in required_dirs if not os.path.isdir(d)]
          if missing:
              print(f'Missing directories: {missing}')
              sys.exit(1)
          print('All required directories exist')
          "

      - name: Verify script files
        run: |
          python -c "
          import os, sys
          required_files = [
              'src/scripts/train_pipeline.py',
              'src/scripts/evaluate_pipeline.py',
              'src/scripts/deploy_api.py',
              'src/scripts/model_monitoring.py'
          ]
          missing = [f for f in required_files if not os.path.isfile(f)]
          if missing:
              print(f'Missing script files: {missing}')
              sys.exit(1)
          print('All required script files exist')
          "

      - name: Validate workflow file
        run: |
          pip install pyyaml
          python -c "
          import yaml, sys
          try:
              with open('.github/workflows/mlops-pipeline.yml', 'r') as f:
                  yaml.safe_load(f)
              print('Workflow file syntax is valid')
          except yaml.YAMLError as e:
              print(f'Invalid workflow syntax: {e}')
              sys.exit(1)
          "

  test-data-pipeline:
    name: Test Data Pipeline
    runs-on: ubuntu-latest
    needs: test-infrastructure
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
          pip install pytest pytest-cov

      # Create minimal test data if not exists
      - name: Create test data if needed
        run: |
          mkdir -p data
          if [ ! -f "data/data.csv" ]; then
            echo "Creating minimal test dataset"
            python -c "
            import pandas as pd
            import numpy as np

            # Create synthetic data for water quality
            np.random.seed(42)
            n_samples = 100

            data = pd.DataFrame({
                'ph': np.random.uniform(6.0, 8.5, n_samples),
                'Hardness': np.random.uniform(100, 300, n_samples),
                'Solids': np.random.uniform(5000, 15000, n_samples),
                'Chloramines': np.random.uniform(4, 9, n_samples),
                'Sulfate': np.random.uniform(200, 400, n_samples),
                'Conductivity': np.random.uniform(300, 500, n_samples),
                'Organic_carbon': np.random.uniform(5, 15, n_samples),
                'Trihalomethanes': np.random.uniform(50, 100, n_samples),
                'Turbidity': np.random.uniform(2, 6, n_samples),
                'Potability': np.random.randint(0, 2, n_samples)
            })

            # Add some missing values to test preprocessing
            for col in data.columns:
                if col != 'Potability':
                    mask = np.random.random(n_samples) < 0.05  # 5% missing values
                    data.loc[mask, col] = np.nan

            data.to_csv('data/data.csv', index=False)
            print('Test dataset created with shape:', data.shape)
            "
          fi

      - name: Run preprocessing tests
        run: python -m pytest src/tests/test_basic.py -v

      - name: Test data preprocessing
        run: |
          python -c "
          import pandas as pd
          import numpy as np
          import os
          import sys

          # Test train_pipeline preprocessing functionality
          try:
              data = pd.read_csv('data/data.csv')
              print(f'Original data shape: {data.shape}')

              # Handle missing values with median imputation
              clean_data = data.copy()
              for column in clean_data.columns:
                  if column != 'Potability':
                      clean_data[column] = clean_data[column].fillna(clean_data[column].median())

              # Verify no missing values remain
              if clean_data.isnull().sum().sum() > 0:
                  print('Error: Missing values remain after preprocessing')
                  sys.exit(1)

              # Export test data
              os.makedirs('data', exist_ok=True)
              clean_data.to_csv('data/clean_data.csv', index=False)

              # Create train/test split for testing
              from sklearn.model_selection import train_test_split
              X = clean_data.drop('Potability', axis=1)
              y = clean_data['Potability']
              X_train, X_test, y_train, y_test = train_test_split(
                  X, y, test_size=0.2, random_state=42, stratify=y)

              # Export train and test sets
              train_df = pd.concat([X_train, y_train], axis=1)
              test_df = pd.concat([X_test, y_test], axis=1)
              train_df.to_csv('data/train_data.csv', index=False)
              test_df.to_csv('data/test_data.csv', index=False)

              print('Data preprocessing test passed successfully!')
          except Exception as e:
              print(f'Data preprocessing failed: {str(e)}')
              sys.exit(1)
          "

  test-model-training:
    name: Test Model Training
    runs-on: ubuntu-latest
    needs: test-data-pipeline
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
          pip install pytest pytest-cov scikit-learn xgboost lightgbm

      # Create directories if they don't exist
      - name: Setup directories
        run: |
          mkdir -p models
          mkdir -p results

      - name: Test basic model training
        run: |
          python -c "
          import pandas as pd
          import numpy as np
          import os
          import pickle
          import sys
          from sklearn.ensemble import RandomForestClassifier

          try:
              # Check if test data exists, if not exit gracefully
              if not os.path.exists('data/train_data.csv'):
                  print('Test data not available, skipping model training test')
                  sys.exit(0)

              # Load data
              train_data = pd.read_csv('data/train_data.csv')
              X_train = train_data.drop('Potability', axis=1)
              y_train = train_data['Potability']

              # Train a simple model
              print('Training test Random Forest model...')
              model = RandomForestClassifier(n_estimators=10, random_state=42)
              model.fit(X_train, y_train)

              # Save the model
              os.makedirs('models', exist_ok=True)
              with open('models/random_forest_model.pkl', 'wb') as f:
                  pickle.dump(model, f)

              print('Model training test passed successfully!')

              # Export feature importances
              os.makedirs('results', exist_ok=True)
              feature_importance = pd.DataFrame({
                  'Feature': X_train.columns,
                  'Importance': model.feature_importances_
              }).sort_values('Importance', ascending=False)
              feature_importance.to_csv('results/feature_importance_test.csv', index=False)

              print('Feature importance exported successfully!')
          except Exception as e:
              print(f'Model training test failed: {str(e)}')
              sys.exit(1)
          "

  test-model-evaluation:
    name: Test Model Evaluation
    runs-on: ubuntu-latest
    needs: test-model-training
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
          pip install pytest pytest-cov scikit-learn

      - name: Test model evaluation
        run: |
          python -c "
          import pandas as pd
          import numpy as np
          import os
          import pickle
          import json
          import sys
          from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

          try:
              # Check if necessary files exist
              if not os.path.exists('models/random_forest_model.pkl') or not os.path.exists('data/test_data.csv'):
                  print('Model or test data not available, skipping evaluation test')
                  sys.exit(0)

              # Load model
              with open('models/random_forest_model.pkl', 'rb') as f:
                  model = pickle.load(f)

              # Load test data
              test_data = pd.read_csv('data/test_data.csv')
              X_test = test_data.drop('Potability', axis=1)
              y_test = test_data['Potability']

              # Make predictions
              y_pred = model.predict(X_test)

              # Calculate metrics
              metrics = {
                  'accuracy': float(accuracy_score(y_test, y_pred)),
                  'precision': float(precision_score(y_test, y_pred)),
                  'recall': float(recall_score(y_test, y_pred)),
                  'f1_score': float(f1_score(y_test, y_pred))
              }

              # Save metrics
              os.makedirs('results', exist_ok=True)
              with open('results/evaluation_results_test.json', 'w') as f:
                  json.dump(metrics, f, indent=4)

              # Save best model (just a copy for testing)
              with open('results/best_model_test.pkl', 'wb') as f:
                  pickle.dump(model, f)

              print('Model evaluation test completed successfully!')
              print(f'Metrics: {metrics}')
          except Exception as e:
              print(f'Model evaluation test failed: {str(e)}')
              sys.exit(1)
          "

      - name: Run comprehensive model tests
        run: python -m pytest src/tests/test_water-potability-detection.py -v

  test-api:
    name: Test API Functionality
    runs-on: ubuntu-latest
    needs: test-model-evaluation
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
          pip install pytest pytest-cov fastapi uvicorn pytest-asyncio httpx

      - name: Test API functionality
        run: |
          # Copy model to expected location for API
          mkdir -p results
          cp models/random_forest_model.pkl results/best_model.pkl 2>/dev/null || true

          # Run basic API tests if file exists
          if [ -f "src/tests/test_ab_testing.py" ]; then
            python -m pytest src/tests/test_ab_testing.py -v
          fi

          if [ -f "src/tests/test_mlops_pipeline.py" ]; then
            python -m pytest src/tests/test_mlops_pipeline.py -v
          fi

  run-all-tests:
    name: Run All Unit Tests
    runs-on: ubuntu-latest
    needs:
      [
        test-infrastructure,
        test-data-pipeline,
        test-model-training,
        test-model-evaluation,
        test-api,
      ]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.12"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .
          pip install pytest pytest-cov pytest-html

      - name: Run all tests with coverage
        run: |
          python -m pytest src/tests/ --cov=src --cov-report=xml --cov-report=html --html=test-report.html
        continue-on-error: true

      - name: Upload test reports
        uses: actions/upload-artifact@v4
        with:
          name: test-reports
          path: |
            coverage.xml
            htmlcov/
            test-report.html

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: run-all-tests
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download test reports
        uses: actions/download-artifact@v4
        with:
          name: test-reports

      - name: Display test summary
        run: |
          echo "## MLOps Pipeline Test Summary" > test_summary.md
          echo "" >> test_summary.md
          echo "✅ Infrastructure tests completed" >> test_summary.md
          echo "✅ Data pipeline tests completed" >> test_summary.md
          echo "✅ Model training tests completed" >> test_summary.md
          echo "✅ Model evaluation tests completed" >> test_summary.md
          echo "✅ API tests completed" >> test_summary.md
          echo "" >> test_summary.md
          echo "For detailed results, see the test reports in the workflow artifacts." >> test_summary.md

          cat test_summary.md

      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: test-summary
          path: test_summary.md
